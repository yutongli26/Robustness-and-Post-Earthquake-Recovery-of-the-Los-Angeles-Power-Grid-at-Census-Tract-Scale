import os
import geopandas as gpd
import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from mpl_toolkits.axes_grid1 import make_axes_locatable
from pathlib import Path
import math

# =============================================================================
# Project paths (portable)
# =============================================================================
# Project root = directory containing this script.
PROJECT_ROOT = Path(__file__).resolve().parent

# All inputs are stored under ./Data (relative to PROJECT_ROOT).
DATA_DIR = PROJECT_ROOT / "Data"

# All stage output folders live under the project root by convention.
OUTPUT_ROOT = str(PROJECT_ROOT)

# =============================================================================
# Inputs (Data/)
# =============================================================================
DEVICES_CSV: str = str(DATA_DIR / "LA_Substations_WithFragility_UPDATED_CEC.csv")
SHAPEFILE_PATH: str = str(DATA_DIR / "LA_Tracts_With_Population.shp")
SCENARIOS = ["Northridge", "SanFernando", "LongBeach"]

# Topology products (generated by "Topology and Weight.py" and consumed here)
CEC_GRAPH_EDGES_CSV: str = str(DATA_DIR / "substation_graph_CEC_edges.csv")
CEC_TRANSMISSION_SHP: str = str(DATA_DIR / "TransmissionLine_CEC.shp")

# City boundary
CITY_BOUNDARY_SHP: str = str(DATA_DIR / "City_Boundary.shp")

# =============================================================================
# Global plotting style (kept as module-level side effects)
# =============================================================================
plt.style.use("seaborn-v0_8-whitegrid")
plt.rcParams["font.family"] = "sans-serif"
plt.rcParams["figure.dpi"] = 150
plt.rcParams["savefig.dpi"] = 300
plt.rcParams["axes.unicode_minus"] = False

# ==============================================================================
# Helpers
# ==============================================================================
def ensure_dir(directory: str) -> bool:
    """Return True if directory exists; otherwise warn and return False."""
    if not os.path.exists(directory):
        print(f"  [Warning] Directory not found: {directory}")
        return False
    return True

def save_plot(fig: plt.Figure, folder: str, filename: str) -> None:
    """Save and close a matplotlib figure."""
    path = os.path.join(folder, filename)
    fig.savefig(path, bbox_inches="tight")
    plt.close(fig)
    print(f"  [Saved] {filename}")

def load_shapefile():
    """
    Load the tract shapefile and standardize the tract id column name to 'tract_id'.
    Returns:
        GeoDataFrame or None
    """
    if not os.path.exists(SHAPEFILE_PATH):
        print(f"  [GIS Skip] Shapefile not found at {SHAPEFILE_PATH}")
        return None

    try:
        gdf = gpd.read_file(SHAPEFILE_PATH)

        possible_cols = ["GEOID", "GEOID10", "GEOID20", "TRACTCE", "TRACT", "geoid"]
        for c in possible_cols:
            if c in gdf.columns:
                gdf["tract_id"] = gdf[c].astype(str).str.strip()
                break

        if "tract_id" not in gdf.columns:
            print("  [GIS Warning] Could not identify Tract ID column in shapefile.")
            return None

        return gdf

    except Exception as e:
        print(f"  [GIS Error] Failed to load shapefile: {e}")
        return None

def plot_map(
    gdf,
    data_series,
    title: str,
    output_dir: str,
    filename: str,
    cmap: str = "OrRd",
    label: str = "Value",
    vmin=None,
    vmax=None,
    cmap_low: float = 0.1,
    cmap_high: float = 1.0,
):
    """
    Enhanced mapping utility:
      1) Robust tract-id matching with multiple strategies.
      2) Optional shared vmin/vmax (if provided).
      3) Truncated colormap (cuts off very light colors).
      4) Larger title/label/ticks for publication-ready maps.
    """
    if gdf is None:
        return

    # ---- Prepare data ----
    data_df = data_series.to_frame(name="val")
    data_df.index = (
        data_df.index.astype(str)
        .str.split(".").str[0]
        .str.strip()
    )

    gdf["tract_id"] = (
        gdf["tract_id"].astype(str)
        .str.split(".").str[0]
        .str.strip()
    )

    print(f"\n[GIS Debug] Trying to map: {title}")

    # ---- Matching strategies ----
    matched_gdf = None
    best_match_count = 0

    strategies = [
        ("Direct", lambda s: s),
        ("CSV zfill 11", lambda s: s.str.zfill(11)),
        ("Shapefile zfill 11", None),
        ("Suffix 6", lambda s: s.str[-6:]),
    ]

    for name, func in strategies:
        temp_data = data_df.copy()
        temp_gdf = gdf.copy()

        if name == "Shapefile zfill 11":
            temp_gdf["tract_id"] = temp_gdf["tract_id"].str.zfill(11)
        elif func:
            temp_data.index = func(temp_data.index)

        merged = temp_gdf.merge(
            temp_data,
            left_on="tract_id",
            right_index=True,
            how="inner",
        )

        count = len(merged)
        if count > best_match_count:
            best_match_count = count
            matched_gdf = merged
            print(f"   > Strategy '{name}' matched: {count} tracts")

    if best_match_count == 0 or matched_gdf is None:
        print("   ❌ [GIS Error] ALL MATCHING STRATEGIES FAILED.")
        return

    print(f"   ✅ Final Map Area: {best_match_count} tracts.")

    # ---- Plot ----
    fig, ax = plt.subplots(figsize=(10, 10))

    # vmin/vmax based on finite values only
    data_vals = matched_gdf["val"].astype(float)
    data_vals_valid = data_vals[np.isfinite(data_vals)]
    if vmin is None:
        vmin = float(data_vals_valid.min())
    if vmax is None:
        vmax = float(data_vals_valid.max())

    # Truncated colormap to avoid near-white low end
    base_cmap = plt.colormaps.get_cmap(cmap)
    new_colors = base_cmap(np.linspace(cmap_low, cmap_high, 256))
    truncated_cmap = mcolors.LinearSegmentedColormap.from_list(
        f"{cmap}_trunc_{cmap_low}_{cmap_high}",
        new_colors,
    )

    matched_gdf.plot(
        column="val",
        ax=ax,
        legend=False,
        cmap=truncated_cmap,
        vmin=vmin,
        vmax=vmax,
        missing_kwds={
            "color": "lightgrey",
            "edgecolor": "none",
            "hatch": "///",
        },
    )

    # Colorbar
    divider = make_axes_locatable(ax)
    cax = divider.append_axes("right", size="5%", pad=0.1)

    sm = plt.cm.ScalarMappable(
        cmap=truncated_cmap,
        norm=plt.Normalize(vmin=vmin, vmax=vmax),
    )
    sm._A = []  # dummy for ScalarMappable

    ticks = np.linspace(vmin, vmax, 5)
    cbar = fig.colorbar(sm, cax=cax, ticks=ticks)
    cbar.ax.set_yticklabels([f"{t:.2f}" for t in ticks], fontsize=25)
    cbar.set_label(label, fontsize=25)

    ax.set_title(title, fontsize=25, fontweight="bold", pad=12)
    ax.axis("off")

    # Save
    if "save_plot" in globals():
        save_plot(fig, output_dir, filename)
    else:
        import os  # kept to preserve the original structure/behavior

        os.makedirs(output_dir, exist_ok=True)
        plt.savefig(os.path.join(output_dir, filename), bbox_inches="tight")

    plt.close(fig)

def vis_stage1(gdf):
    stage_dir = os.path.join(OUTPUT_ROOT, "Stage 1 Output")
    if not ensure_dir(stage_dir):
        return

    print("--- Visualizing Stage 1 ---")

    # Global style (larger fonts)
    try:
        sns.set_style("whitegrid")
        sns.set_context("talk", font_scale=1.2)
    except Exception:
        plt.style.use("ggplot")

    # -------------------------------------------------------------------------
    # 1) Establish a consistent vmin/vmax for outage probability maps
    # -------------------------------------------------------------------------
    all_probs = []
    for scen in SCENARIOS:
        prob_file = os.path.join(stage_dir, f"MC_TractOutage_Prob_{scen}.csv")
        if not os.path.exists(prob_file):
            continue

        df_tmp = pd.read_csv(prob_file)
        if "peak_outage_prob" in df_tmp.columns:
            all_probs.append(df_tmp["peak_outage_prob"].values)

    if all_probs:
        _ = np.concatenate(all_probs)  # kept for optional strict data-based min/max
        prob_vmin = 0.0
        prob_vmax = 1.0
        # If you ever want strict bounds from data:
        # prob_vmin = float(_.min())
        # prob_vmax = float(_.max())
    else:
        prob_vmin = None
        prob_vmax = None

    # -------------------------------------------------------------------------
    # 2) Per-scenario visualization
    # -------------------------------------------------------------------------
    for scen in SCENARIOS:
        # ===================== Tract Outage Probability ======================
        prob_file = os.path.join(stage_dir, f"MC_TractOutage_Prob_{scen}.csv")
        if os.path.exists(prob_file):
            df = pd.read_csv(prob_file)

            df["tract_id"] = df["tract_id"].astype(str).str.split(".").str[0]
            series = df.set_index("tract_id")["peak_outage_prob"]

            plot_map(
                gdf=gdf,
                data_series=series,
                title=f"Outage Probability - {scen}",
                output_dir=stage_dir,
                filename=f"map_outage_prob_{scen}.png",
                cmap="Reds",
                label="Probability of Power Outage",
                vmin=prob_vmin,
                vmax=prob_vmax,
            )

            # Distribution: histogram + KDE
            fig, ax = plt.subplots(figsize=(8, 5))
            sns.histplot(
                series,
                bins=np.linspace(0, 1, 21),  # 0–1 in 0.05 increments (20 bins)
                stat="density",
                kde=True,
                color="tab:red",
                edgecolor="black",
                alpha=0.6,
                ax=ax,
            )

            ax.set_title(
                f"Distribution of Outage Probabilities - {scen}",
                fontsize=20,
                fontweight="bold",
                pad=10,
            )
            ax.set_xlabel("Probability of power outage", fontsize=18)
            ax.set_ylabel("Density", fontsize=18)
            ax.set_xlim(0.0, 1.0)
            ax.tick_params(axis="both", labelsize=14)

            save_plot(fig, stage_dir, f"hist_outage_prob_{scen}.png")
            plt.close(fig)

        # ===================== Substation Avg Damage State ====================
        dev_file = os.path.join(stage_dir, f"MC_Device_Damage_AvgDS_{scen}.csv")
        if os.path.exists(dev_file):
            df = pd.read_csv(dev_file)

            fig, ax = plt.subplots(figsize=(8, 5))
            sns.histplot(
                data=df,
                x="avg_damage_state",
                bins=np.linspace(0, 4, 17),  # 0–4 in 0.25 increments (16 bins)
                stat="density",
                color="orange",
                edgecolor="black",
                alpha=0.6,
                ax=ax,
            )
            sns.kdeplot(
                data=df,
                x="avg_damage_state",
                color="black",
                linewidth=2,
                ax=ax,
            )

            ax.set_title(
                f"Substation Average Damage State – {scen}",
                fontsize=20,
                fontweight="bold",
                pad=10,
            )
            ax.set_xlabel("Average damage state (0 = none, 4 = collapse)", fontsize=18)
            ax.set_ylabel("Density", fontsize=18)
            ax.set_xlim(0.0, 4.0)
            ax.tick_params(axis="both", labelsize=14)

            save_plot(fig, stage_dir, f"hist_sub_avg_ds_{scen}.png")
            plt.close(fig)


# ==============================================================================
# 2️⃣ Stage 2: Topology
# ==============================================================================
def vis_stage2_topology_with_tracts_latlon():
    """
    High-voltage transmission topology over the City of Los Angeles (lon/lat):

    Layers:
      - City boundary polygon (CITY_BOUNDARY_SHP) as clip/extent reference
      - Census tracts basemap (clipped to city)
      - Original CEC transmission lines (clipped to city bbox)
      - Stage 2 simplified topology edges (CEC_GRAPH_EDGES_CSV)
      - CEC substations (DEVICES_CSV), with robust ID matching
    """
    stage_dir = os.path.join(OUTPUT_ROOT, "Stage 2 Output")
    if not ensure_dir(stage_dir):
        return

    print("--- Visualizing Stage 2 topology (Original Style + Fixed IDs) ---")

    # -------------------------------------------------------------------------
    # 1) Input validation
    # -------------------------------------------------------------------------
    required_paths = [
        (CEC_GRAPH_EDGES_CSV, "CEC_GRAPH_EDGES_CSV"),
        (DEVICES_CSV, "DEVICES_CSV"),
        (SHAPEFILE_PATH, "SHAPEFILE_PATH"),
        (CEC_TRANSMISSION_SHP, "CEC_TRANSMISSION_SHP"),
        (CITY_BOUNDARY_SHP, "CITY_BOUNDARY_SHP"),
    ]
    for path, name in required_paths:
        if not os.path.exists(path):
            print(f"[Error] {name} not found: {path}")
            return

    # -------------------------------------------------------------------------
    # 2) Load GIS basemap layers (city boundary + tracts) in WGS84 (EPSG:4326)
    # -------------------------------------------------------------------------
    city = gpd.read_file(CITY_BOUNDARY_SHP)
    tracts = gpd.read_file(SHAPEFILE_PATH)

    if city.crs is None:
        city = city.set_crs(epsg=3857)
    city = city.to_crs(epsg=4326)

    if tracts.crs is None:
        tracts = tracts.set_crs(epsg=4326)
    else:
        tracts = tracts.to_crs(epsg=4326)

    # Clip tracts to city boundary (fallback to intersects if clip fails)
    try:
        tracts_la = gpd.clip(tracts, city)
    except Exception:
        tracts_la = tracts[tracts.geometry.intersects(city.unary_union)].copy()

    # -------------------------------------------------------------------------
    # 3) Load topology edges + devices; robust ID matching for coordinates
    # -------------------------------------------------------------------------
    edf = pd.read_csv(CEC_GRAPH_EDGES_CSV)
    devices = pd.read_csv(DEVICES_CSV)

    import re

    def robust_id_clean(val):
        """Normalize IDs like 661.0 -> '661' and 'SS0661' -> '661'."""
        s = str(val).strip()

        # Try numeric cleanup first
        try:
            f = float(s)
            if f.is_integer():
                return str(int(f))
        except Exception:
            pass

        # Otherwise strip non-digits and remove leading zeros
        digits = re.sub(r"\D", "", s)
        return digits.lstrip("0") if digits else s

    # Edge-side ID universe (use u column)
    edge_ids = set(edf["u"].apply(robust_id_clean))

    # Find best-matching device ID column by overlap with edge IDs
    best_col = None
    max_overlap = 0
    priority_cols = ["HIFLD_ID", "substation_id", "OBJECTID", "id"]
    all_cols = priority_cols + [c for c in devices.columns if c not in priority_cols]

    for col in all_cols:
        if col not in devices.columns:
            continue
        try:
            dev_ids = devices[col].apply(robust_id_clean)
            overlap = len(edge_ids.intersection(set(dev_ids)))
            if overlap > max_overlap:
                max_overlap = overlap
                best_col = col
                if overlap > len(edge_ids) * 0.9:
                    break  # good enough match
        except Exception:
            continue

    if not (best_col and max_overlap > 0):
        print(f"   ❌ Match Failed. Max overlap: {max_overlap}")
        return

    print(f"   ✅ Smart Match: Using column '{best_col}' (Matches {max_overlap} edges)")
    devices["id_match"] = devices[best_col].apply(robust_id_clean)

    # Clean edge endpoint IDs for mapping
    edf["u_clean"] = edf["u"].apply(robust_id_clean)
    edf["v_clean"] = edf["v"].apply(robust_id_clean)

    # Identify lat/lon fields in devices
    lat_col = next((c for c in ["lat", "Latitude", "Lat", "LAT"] if c in devices.columns), None)
    lon_col = next((c for c in ["lon", "Longitude", "Lon", "LON"] if c in devices.columns), None)
    if not lat_col or not lon_col:
        print("[Error] lat/lon columns not found in devices.")
        return

    devices[lat_col] = pd.to_numeric(devices[lat_col], errors="coerce")
    devices[lon_col] = pd.to_numeric(devices[lon_col], errors="coerce")
    devices = devices.dropna(subset=[lat_col, lon_col])

    lat_map = dict(zip(devices["id_match"], devices[lat_col]))
    lon_map = dict(zip(devices["id_match"], devices[lon_col]))

    edf["lat_u"] = edf["u_clean"].map(lat_map)
    edf["lon_u"] = edf["u_clean"].map(lon_map)
    edf["lat_v"] = edf["v_clean"].map(lat_map)
    edf["lon_v"] = edf["v_clean"].map(lon_map)

    edf = edf.dropna(subset=["lat_u", "lon_u", "lat_v", "lon_v"])
    if edf.empty:
        print("   [Error] No edges with valid coordinates.")
        return

    # -------------------------------------------------------------------------
    # 4) Construct GeoDataFrames for nodes + simplified edges (straight segments)
    # -------------------------------------------------------------------------
    from shapely.geometry import LineString

    g_nodes = gpd.GeoDataFrame(
        devices,
        geometry=gpd.points_from_xy(devices[lon_col], devices[lat_col]),
        crs="EPSG:4326",
    )

    edge_geoms = [
        LineString([(row["lon_u"], row["lat_u"]), (row["lon_v"], row["lat_v"])])
        for _, row in edf.iterrows()
    ]
    g_edges = gpd.GeoDataFrame(edf, geometry=edge_geoms, crs="EPSG:4326")

    # -------------------------------------------------------------------------
    # 5) Load original CEC transmission lines and clip to city bbox (in lon/lat)
    # -------------------------------------------------------------------------
    cec_lines = gpd.read_file(CEC_TRANSMISSION_SHP)
    if cec_lines.crs is None:
        cec_lines = cec_lines.set_crs(epsg=3857)
    cec_lines = cec_lines.to_crs(epsg=4326)

    xmin, ymin, xmax, ymax = city.total_bounds
    cec_lines_la = cec_lines.cx[xmin:xmax, ymin:ymax]

    # -------------------------------------------------------------------------
    # 6) Plot
    # -------------------------------------------------------------------------
    fig, ax = plt.subplots(figsize=(8, 10))

    # City boundary
    city.plot(
        ax=ax,
        facecolor="none",
        edgecolor="#bdbdbd",
        linewidth=1.2,
        zorder=0,
    )

    # Tracts basemap
    tracts_la.plot(
        ax=ax,
        facecolor="none",
        edgecolor="#f0f0f0",
        linewidth=0.3,
        alpha=0.3,
        zorder=0.5,
    )

    # Original CEC lines
    if not cec_lines_la.empty:
        cec_lines_la.plot(
            ax=ax,
            color="#737373",
            linewidth=0.7,
            alpha=0.95,
            zorder=1,
            label="CEC transmission lines",
        )

    # Simplified topology edges
    g_edges.plot(
        ax=ax,
        color="#1f77b4",
        linewidth=1.0,
        alpha=0.95,
        zorder=2,
        label="Simplified topology",
    )

    # Substations
    g_nodes.plot(
        ax=ax,
        marker="x",
        color="#e6550d",
        markersize=40,
        linewidth=1.0,
        zorder=3,
        label="CEC substations",
    )

    ax.set_title(
        "High-Voltage Transmission Network and Substations – City of Los Angeles",
        fontsize=16,
        pad=14,
    )
    ax.set_xlabel("Longitude")
    ax.set_ylabel("Latitude")
    ax.set_aspect("equal", "box")

    # Deduplicate legend entries
    handles, labels = ax.get_legend_handles_labels()
    seen = set()
    uniq = []
    for h, l in zip(handles, labels):
        if l not in seen:
            seen.add(l)
            uniq.append((h, l))
    if uniq:
        ax.legend(*zip(*uniq), loc="lower left", frameon=True)

    # Viewport based on city bbox with padding
    dx, dy = xmax - xmin, ymax - ymin
    ax.set_xlim(xmin - 0.02 * dx, xmax + 0.02 * dx)
    ax.set_ylim(ymin - 0.02 * dy, ymax + 0.02 * dy)

    ax.grid(False)
    save_plot(fig, stage_dir, "vis_stage2_topology_with_tracts_latlon.png")

def vis_stage2_top10_map():
    stage_dir = os.path.join(OUTPUT_ROOT, "Stage 2 Output")
    if not ensure_dir(stage_dir):
        return

    print("--- Visualizing Stage 2 Top10 on Map (Pure & Simple) ---")

    # -------------------------------------------------------------------------
    # 1) Load centrality table and select Top 10
    # -------------------------------------------------------------------------
    cent_file = os.path.join(stage_dir, "impact_centrality_substations.csv")
    if not os.path.exists(cent_file):
        print(f"[Error] Centrality file not found: {cent_file}")
        return

    df_cent = pd.read_csv(cent_file)
    id_col = df_cent.columns[0]  # default: first column is substation ID

    top10 = df_cent.nlargest(10, "impact_centrality").copy()
    top10["rank"] = range(1, 11)

    # -------------------------------------------------------------------------
    # 2) Load devices table (substations with lat/lon)
    # -------------------------------------------------------------------------
    if not os.path.exists(DEVICES_CSV):
        print(f"[Error] Devices file not found: {DEVICES_CSV}")
        return

    devices = pd.read_csv(DEVICES_CSV)

    # Identify ID column in devices (preserve original fallback order)
    dev_id_col = "HIFLD_ID"
    if dev_id_col not in devices.columns:
        if "id" in devices.columns:
            dev_id_col = "id"
        elif "OBJECTID" in devices.columns:
            dev_id_col = "OBJECTID"

    # Identify lat/lon columns
    lat_col = next(
        (c for c in ["lat", "Latitude", "Lat", "latitude"] if c in devices.columns),
        None,
    )
    lon_col = next(
        (c for c in ["lon", "Longitude", "Lon", "longitude"] if c in devices.columns),
        None,
    )
    if lat_col is None or lon_col is None:
        print("[Error] lat/lon columns not found in devices.")
        return

    # -------------------------------------------------------------------------
    # 3) Merge Top10 with devices via simple string join key
    # -------------------------------------------------------------------------
    def _clean_id_series(s: pd.Series) -> pd.Series:
        return s.astype(str).str.strip().str.replace(".0", "", regex=False)

    top10["join_key"] = _clean_id_series(top10[id_col])
    devices["join_key"] = _clean_id_series(devices[dev_id_col])

    merged = top10.merge(devices, on="join_key", how="left")

    # -------------------------------------------------------------------------
    # 4) Load basemap (tract boundaries) if available
    # -------------------------------------------------------------------------
    tracts = None
    if os.path.exists(SHAPEFILE_PATH):
        try:
            tracts = gpd.read_file(SHAPEFILE_PATH).to_crs(epsg=4326)
        except Exception:
            tracts = None

    # GeoDataFrame: all substations (gray points)
    all_subs_gdf = gpd.GeoDataFrame(
        devices,
        geometry=gpd.points_from_xy(devices[lon_col], devices[lat_col]),
        crs="EPSG:4326",
    )

    # GeoDataFrame: top 10 (gold stars)
    top10_gdf = gpd.GeoDataFrame(
        merged,
        geometry=gpd.points_from_xy(merged[lon_col], merged[lat_col]),
        crs="EPSG:4326",
    )

    # -------------------------------------------------------------------------
    # 5) Plot
    # -------------------------------------------------------------------------
    fig, ax = plt.subplots(figsize=(6, 8.5))

    # Tract boundaries basemap
    if tracts is not None and not tracts.empty:
        tracts.boundary.plot(
            ax=ax,
            color="#d0d0d0",
            linewidth=0.5,
            zorder=0,
        )

    # All substations
    all_subs_gdf.plot(
        ax=ax,
        marker="o",
        facecolor="#808080",
        edgecolor="none",
        markersize=12,
        alpha=0.30,
        label="All substations",
        zorder=1,
    )

    # Top10 substations
    if not top10_gdf.empty:
        top10_gdf.plot(
            ax=ax,
            marker="*",
            markersize=200,
            facecolor="#f4a300",
            edgecolor="black",
            linewidth=0.8,
            label="Top10 impact",
            zorder=2,
        )

        # Rank annotations
        for _, row in top10_gdf.iterrows():
            if pd.notnull(row.geometry):
                ax.annotate(
                    text=str(row["rank"]),
                    xy=(row.geometry.x, row.geometry.y),
                    xytext=(4, 4),
                    textcoords="offset points",
                    fontsize=11,
                    fontweight="bold",
                    color="black",
                )

    # -------------------------------------------------------------------------
    # 6) Set extent (prefer city boundary bbox, then substations, then tracts)
    # -------------------------------------------------------------------------
    bounds_found = False

    if "CITY_BOUNDARY_SHP" in globals() and os.path.exists(CITY_BOUNDARY_SHP):
        city = gpd.read_file(CITY_BOUNDARY_SHP)
        if city.crs is None:
            city = city.set_crs(epsg=3857)
        city = city.to_crs(epsg=4326)
        minx, miny, maxx, maxy = city.total_bounds
        bounds_found = True
    elif not all_subs_gdf.empty:
        minx, miny, maxx, maxy = all_subs_gdf.total_bounds
        bounds_found = True
    elif tracts is not None and not tracts.empty:
        minx, miny, maxx, maxy = tracts.total_bounds
        bounds_found = True

    if not bounds_found:
        print("[vis_stage2_top10_map] No geometry bounds found.")
        plt.close(fig)
        return

    dx = (maxx - minx) * 0.05
    dy = (maxy - miny) * 0.05
    ax.set_xlim(minx - dx, maxx + dx)
    ax.set_ylim(miny - dy, maxy + dy)

    # -------------------------------------------------------------------------
    # 7) Title, legend, and summary table text
    # -------------------------------------------------------------------------
    ax.set_title(
        "Top 10 Critical Substations (Impact Centrality)",
        fontsize=20,
        pad=10,
    )
    ax.set_axis_off()

    leg = ax.legend(loc="upper right", frameon=True, fontsize=11)
    frame = leg.get_frame()
    frame.set_facecolor("white")
    frame.set_edgecolor("#cccccc")
    frame.set_linewidth(1.2)
    frame.set_alpha(0.95)

    table_text = "rank  id          impact\n"
    for _, row in top10.iterrows():
        impact_str = f"{row['impact_centrality']:.0f}"
        table_text += f"{row['rank']:<5} {str(row['join_key']):<11} {impact_str}\n"

    plt.text(
        0.02,
        0.02,
        table_text,
        transform=ax.transAxes,
        family="monospace",
        fontsize=9,
        va="bottom",
        ha="left",
        bbox=dict(
            boxstyle="round,pad=0.4",
            fc="white",
            ec="#cccccc",
            alpha=0.95,
        ),
    )

    # -------------------------------------------------------------------------
    # 8) Save
    # -------------------------------------------------------------------------
    out_path = os.path.join(stage_dir, "vis_stage2_top10_map.png")
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close(fig)
    print(f"  -> Saved: {out_path}")

def vis_stage2():
    stage_dir = os.path.join(OUTPUT_ROOT, "Stage 2 Output")
    if not ensure_dir(stage_dir):
        return

    print("--- Visualizing Stage 2 (2.5) ---")

    # ------------------------------------------------------------------
    # 0) Map-based figures
    # ------------------------------------------------------------------
    vis_stage2_top10_map()
    vis_stage2_topology_with_tracts_latlon()

    # ------------------------------------------------------------------
    # Helpers (local to Stage 2 visualization)
    # ------------------------------------------------------------------
    def choose_x_axis(df: pd.DataFrame, fallback_col: str) -> str:
        """Choose a robust x-axis column; otherwise create a normalized fallback."""
        if "fraction_removed" in df.columns:
            return "fraction_removed"
        if "nodes_removed" in df.columns:
            return "nodes_removed"

        df[fallback_col] = np.arange(len(df)) / max(len(df) - 1, 1)
        return fallback_col

    def is_coord_like(colname: str) -> bool:
        """Match only when the entire col or underscore-bound variants indicate coordinates."""
        cl = colname.lower()
        coord_keywords = [
            "lat", "lon", "latitude", "longitude",
            "xcoord", "ycoord", "x_coord", "y_coord",
        ]
        return any(
            cl == kw or cl.endswith("_" + kw) or cl.startswith(kw + "_")
            for kw in coord_keywords
        )

    def detect_id_col(df: pd.DataFrame) -> str:
        """Heuristically find a substation id column; fallback to the first column."""
        for c in df.columns:
            cl = c.lower()
            if "substation" in cl and "id" in cl:
                return c
        return df.columns[0]

    def detect_centrality_metrics(df: pd.DataFrame, id_col: str) -> list[str]:
        """Auto-discover centrality-like numeric columns, excluding ID and coordinate fields."""
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

        blacklist = {id_col}
        for c in df.columns:
            if is_coord_like(c):
                blacklist.add(c)

        auto_metrics = []
        for c in numeric_cols:
            if c in blacklist:
                continue

            cl = c.lower()
            if (
                "centrality" in cl
                or c in [
                    "degree",
                    "impact_centrality",
                    "impact_population",
                    "lambda2_impact",
                    "closeness_centrality",
                ]
            ):
                auto_metrics.append(c)

        return sorted(set(auto_metrics))

    # ------------------------------------------------------------------
    # 1) Percolation curves: render each percolation_curve_*.csv
    # ------------------------------------------------------------------
    for fname in os.listdir(stage_dir):
        if not (fname.startswith("percolation_curve_") and fname.endswith(".csv")):
            continue

        fpath = os.path.join(stage_dir, fname)
        try:
            dfp = pd.read_csv(fpath)
        except Exception as e:
            print(f"[Stage2 vis] skip {fname}: {e}")
            continue

        if "lcc_fraction" not in dfp.columns:
            print(f"[Stage2 vis] {fname} missing lcc_fraction, skip.")
            continue

        x_col = choose_x_axis(dfp, fallback_col="_fraction_removed_fallback")

        fig, ax = plt.subplots(figsize=(8, 6))
        ax.plot(dfp[x_col], dfp["lcc_fraction"], "o-", markersize=4)
        ax.set_title(f"Percolation Analysis ({fname.replace('.csv', '')})")
        ax.set_ylabel("LCC Fraction")
        ax.set_xlabel(x_col)
        save_plot(fig, stage_dir, f"vis_stage2_{fname.replace('.csv', '')}.png")

    # ------------------------------------------------------------------
    # 1b) Overlay comparison: impact vs random (if both files exist)
    # ------------------------------------------------------------------
    impact_csv = os.path.join(stage_dir, "percolation_curve_impact.csv")
    rand_csv = os.path.join(stage_dir, "percolation_curve_random.csv")

    if os.path.exists(impact_csv) and os.path.exists(rand_csv):
        df_imp = pd.read_csv(impact_csv)
        df_rand = pd.read_csv(rand_csv)

        x_imp_col = choose_x_axis(df_imp, fallback_col="_x_fallback")
        x_rand_col = choose_x_axis(df_rand, fallback_col="_x_fallback")

        fig, ax = plt.subplots(figsize=(8, 6))
        ax.plot(
            df_imp[x_imp_col],
            df_imp["lcc_fraction"],
            marker="o",
            linestyle="-",
            label="Impact (λ2-targeted)",
        )
        ax.plot(
            df_rand[x_rand_col],
            df_rand["lcc_fraction"],
            marker="s",
            linestyle="--",
            label="Random",
        )
        ax.set_ylabel("LCC Fraction")
        ax.set_xlabel(x_imp_col)
        ax.set_title("Percolation Comparison: Impact vs Random")
        ax.legend()
        save_plot(fig, stage_dir, "vis_stage2_percolation_compare_impact_vs_random.png")

    # ------------------------------------------------------------------
    # 2) Centrality metrics: distributions, scatters, top10 bars, corr heatmap
    # ------------------------------------------------------------------
    cent_file = os.path.join(stage_dir, "impact_centrality_substations.csv")
    if not os.path.exists(cent_file):
        print("[Stage2 vis] centrality file not found.")
        return

    df = pd.read_csv(cent_file)
    id_col = detect_id_col(df)

    valid_metrics = detect_centrality_metrics(df, id_col=id_col)
    if not valid_metrics:
        print("[Stage2 vis] no valid centrality columns found.")
        return

    print(f"[Stage2 vis] Detected centrality metrics: {valid_metrics}")

    # 2.1) Distributions (hist)
    for m in valid_metrics:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.hist(df[m].dropna().values, bins=40)
        ax.set_title(f"Distribution of {m}")
        ax.set_xlabel(m)
        ax.set_ylabel("Count")
        save_plot(fig, stage_dir, f"vis_stage2_dist_{m}.png")

    # 2.2-a) Scatter: impact_centrality vs others
    if "impact_centrality" in valid_metrics and len(valid_metrics) > 1:
        for m in valid_metrics:
            if m == "impact_centrality":
                continue
            fig, ax = plt.subplots(figsize=(6.5, 6))
            ax.scatter(df[m], df["impact_centrality"], alpha=0.4, s=12)
            ax.set_xlabel(m)
            ax.set_ylabel("impact_centrality")
            ax.set_title(f"impact_centrality vs {m}")
            save_plot(fig, stage_dir, f"vis_stage2_scatter_impact_vs_{m}.png")

    # 2.2-b) Scatter: impact_population vs others
    if "impact_population" in valid_metrics and len(valid_metrics) > 1:
        for m in valid_metrics:
            if m == "impact_population":
                continue
            fig, ax = plt.subplots(figsize=(6.5, 6))
            ax.scatter(df[m], df["impact_population"], alpha=0.4, s=12)
            ax.set_xlabel(m)
            ax.set_ylabel("impact_population")
            ax.set_title(f"impact_population vs {m}")
            save_plot(fig, stage_dir, f"vis_stage2_scatter_impopp_vs_{m}.png")

    # 2.3) Top10 for each metric
    for m in valid_metrics:
        top10 = df.nlargest(10, m).copy()
        top10[id_col] = top10[id_col].astype(str)
        top10 = top10.sort_values(m, ascending=False)

        fig, ax = plt.subplots(figsize=(10, 6))
        sns.barplot(data=top10, x=id_col, y=m, ax=ax, errorbar=None)

        ax.set_title(f"Top 10 Substations by {m}")
        ax.set_xlabel(id_col)
        ax.set_ylabel(m)
        ax.tick_params(axis="x", labelrotation=45)
        for lab in ax.get_xticklabels():
            lab.set_horizontalalignment("right")

        save_plot(fig, stage_dir, f"vis_stage2_top10_{m}.png")

    # 2.4) Correlation heatmap
    if len(valid_metrics) > 1:
        corr = df[valid_metrics].corr()
        fig, ax = plt.subplots(figsize=(max(6, 0.7 * len(valid_metrics)), 5))
        sns.heatmap(corr, annot=True, fmt=".2f", ax=ax)
        ax.set_title("Centrality Metric Correlations")
        save_plot(fig, stage_dir, "vis_stage2_centrality_corr_heatmap.png")


# ==============================================================================
# 3️⃣ Stage 3: Recovery
# ==============================================================================
TIME_LIMIT_HR = 240.0


# ------------------------------------------------------------------------------
# Helpers
# ------------------------------------------------------------------------------
def save_plot(fig, folder: str, filename: str) -> None:
    out_path = os.path.join(folder, filename)
    plt.savefig(out_path, dpi=300, bbox_inches="tight")
    plt.close(fig)
    print(f"  -> Saved: {filename}")


# ------------------------------------------------------------------------------
# Stage 3 Visualization
# ------------------------------------------------------------------------------
def vis_stage3(gdf):
    stage_dir = os.path.join(OUTPUT_ROOT, "Stage 3 Output")
    if not ensure_dir(stage_dir):
        return

    print("--- Visualizing Stage 3 ---")

    for scen in SCENARIOS:
        # ------------------------------------------------------------------
        # 1) Tract KPIs (Map + Hist)
        # ------------------------------------------------------------------
        kpi_file = os.path.join(stage_dir, f"tract_kpis_{scen}.csv")
        if os.path.exists(kpi_file):
            df = pd.read_csv(kpi_file)

            # Rename first column to tract_id (as you had)
            df.rename(columns={df.columns[0]: "tract_id"}, inplace=True)
            df["tract_id"] = df["tract_id"].astype(str).str.split(".").str[0]

            for metric in ["T50", "AUC"]:
                if metric not in df.columns:
                    continue

                # Map
                plot_map(
                    gdf,
                    df.set_index("tract_id")[metric],
                    f"{metric} Map - {scen}",
                    stage_dir,
                    f"map_{metric}_{scen}.png",
                    cmap="viridis_r" if metric == "T50" else "viridis",
                    label=metric,
                )

                # Histogram + KDE
                fig, ax = plt.subplots(figsize=(8, 5))
                sns.histplot(df[metric], bins=30, kde=True, ax=ax)
                ax.set_title(f"{metric} Distribution across Tracts - {scen}")
                save_plot(fig, stage_dir, f"hist_{metric}_{scen}.png")

        # ------------------------------------------------------------------
        # 2) Graph Robustness (Dual Axis + Auto X-Axis Trim)
        # ------------------------------------------------------------------
        rob_file = os.path.join(stage_dir, f"graph_robustness_mean_{scen}.csv")
        if os.path.exists(rob_file):
            df = pd.read_csv(rob_file)

            # Auto X-axis truncation point (as you had)
            max_lcc = df["lcc_size"].max()
            finished_rows = df[df["lcc_size"] >= max_lcc * 0.999]

            if not finished_rows.empty:
                finish_time = finished_rows["t"].iloc[0]
                x_limit = min(df["t"].max(), max(12.0, finish_time * 1.2))
            else:
                x_limit = df["t"].max()

            fig, ax1 = plt.subplots(figsize=(10, 5))

            # Left axis: LCC size
            color = "tab:red"
            ax1.set_xlabel("Time (hours)")
            ax1.set_ylabel("LCC Size (Nodes)", color=color)
            ax1.plot(df["t"], df["lcc_size"], color=color, label="LCC Size", linewidth=2.5)
            ax1.tick_params(axis="y", labelcolor=color)
            ax1.grid(True, linestyle="--", alpha=0.5)
            ax1.set_ylim(bottom=0)

            # Apply auto X range
            ax1.set_xlim(0, x_limit)

            # Right axis: avg_degree
            ax2 = ax1.twinx()
            color = "tab:blue"

            if "avg_degree" in df.columns:
                y_col = "avg_degree"
                y_label = "Average Degree"
            elif "global_efficiency" in df.columns:
                y_col = "global_efficiency"
                y_label = "Global Efficiency"
            else:
                y_col = df.columns[-1]
                y_label = y_col

            ax2.set_ylabel(y_label, color=color)
            ax2.plot(df["t"], df[y_col], color=color, linestyle="--", label=y_label, linewidth=2.5)
            ax2.tick_params(axis="y", labelcolor=color)
            ax2.set_ylim(bottom=0)

            # Combine legends
            lines_1, labels_1 = ax1.get_legend_handles_labels()
            lines_2, labels_2 = ax2.get_legend_handles_labels()
            ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc="lower right")

            plt.title(f"Dynamic Network Robustness - {scen}")
            save_plot(fig, stage_dir, f"vis_stage3_robustness_{scen}.png")


# ==============================================================================
# 4️⃣ Stage 4: Strategy Comparison
# ==============================================================================
# --- CONFIG ---
SCENARIOS = ["Northridge", "SanFernando", "LongBeach"]

RULES = [
    "centrality-first",   # impact_centrality (λ2)
    "betweenness-first",
    "degree-first",
    "closeness-first",
    "hospital-first",
    "impact-first",       # impact_population
    "random",
]

TIME_LIMIT_HR = 240.0

# Legend order (single source of truth)
LEGEND_ORDER = [
    "Theoretical Limit (Unconstrained)",
    "Impact λ2-First (Grid Topology)",
    "Impact-First (Population)",
    "Betweenness-First (Bridges)",
    "Degree-First (Hubs)",
    "Closeness-First (Accessibility)",
    "Hospital-First (Critical Nodes)",
    "Baseline (Random)",
]


def _ordered_legend(ax):
    """Reorder legend entries using LEGEND_ORDER, without renaming labels."""
    handles, labels = ax.get_legend_handles_labels()
    label_to_handle = {lab: h for h, lab in zip(labels, handles)}

    ordered_handles = []
    ordered_labels = []
    for lab in LEGEND_ORDER:
        if lab in label_to_handle:
            ordered_handles.append(label_to_handle[lab])
            ordered_labels.append(lab)

    if not ordered_handles:
        ax.legend(fontsize=15, loc="lower right", framealpha=0.95)
        return

    ax.legend(
        ordered_handles,
        ordered_labels,
        fontsize=15,
        loc="lower right",
        framealpha=0.95,
    )


def _detect_rule_col(df: pd.DataFrame) -> str:
    """Safely detect the 'rule/strategy' column in KPI files."""
    for c in ["rule", "strategy", "method", "heuristic"]:
        if c in df.columns:
            return c
    return df.columns[0]  # fallback


def _safe_lcc_fraction(df: pd.DataFrame):
    """
    Prefer lcc_fraction. If only lcc_size exists, normalize to [0,1] for plotting.
    This avoids baseline plotting failure when Stage 3 schema differs.
    """
    if "lcc_fraction" in df.columns:
        return df["lcc_fraction"]

    if "lcc_size" in df.columns:
        mx = float(df["lcc_size"].max())
        if mx > 0:
            return df["lcc_size"] / mx
        return df["lcc_size"] * 0.0

    return None


def vis_stage4():
    stage4_dir = os.path.join(OUTPUT_ROOT, "Stage 4 Output")
    stage3_dir = os.path.join(OUTPUT_ROOT, "Stage 3 Output")

    if not os.path.exists(stage4_dir):
        print(f"Error: Directory not found: {stage4_dir}")
        return

    print("--- Visualizing Stage 4: Logistics, Topology & Efficiency ---")

    # Plot styling
    try:
        sns.set_style("whitegrid")
        sns.set_context("talk", font_scale=1.05)
    except Exception:
        plt.style.use("ggplot")

    # Style map for rule curves
    style_map = {
        "random": {
            "color": "lightgray",
            "ls": ":",
            "lw": 2.5,
            "label": "Baseline (Random)",
        },
        "centrality-first": {
            "color": "#e41a1c",
            "ls": "-",
            "lw": 3.0,
            "label": "Impact λ2-First (Grid Topo)",
        },
        "impact-first": {
            "color": "#ff7f00",
            "ls": "-",
            "lw": 3.0,
            "label": "Impact-First (Population)",
        },
        "betweenness-first": {
            "color": "#ffd92f",
            "ls": "-",
            "lw": 3.0,
            "label": "Betweenness-First (Bridges)",
        },
        "degree-first": {
            "color": "#4daf4a",
            "ls": "-",
            "lw": 3.0,
            "label": "Degree-First (Hubs)",
        },
        "closeness-first": {
            "color": "#377eb8",
            "ls": "-",
            "lw": 3.0,
            "label": "Closeness-First (Accessibility)",
        },
        "hospital-first": {
            "color": "#555555",
            "ls": "-",
            "lw": 3.0,
            "label": "Hospital-First (Critical Nodes)",
        },
    }

    # ==================================================================
    # PART 1: Logistics Heatmaps
    # ==================================================================
    print("\n[PART 1] Visualizing Logistics Heatmaps...")

    base_to_task_file = os.path.join(stage4_dir, "travel_base_to_task.csv")
    if os.path.exists(base_to_task_file):
        try:
            df_base_task = pd.read_csv(base_to_task_file, index_col=0)

            # NOTE (logic risk): fillna(0) makes missing/unreachable look like 0 travel time.
            df_plot = df_base_task.replace([np.inf, -np.inf], np.nan).fillna(0)

            fig, ax = plt.subplots(figsize=(12, 10))
            sns.heatmap(
                df_plot,
                cmap="YlGnBu",
                cbar_kws={"label": "Travel Time (Hours)"},
                xticklabels=False,
                yticklabels=False,
                ax=ax,
            )
            ax.set_title(
                "Logistics Constraint: Base-to-Task Travel Time (Full Network)",
                fontweight="bold",
                fontsize=24,
                pad=10,
            )
            ax.set_xlabel(f"Destination Substation ID (N={df_plot.shape[1]})", fontsize=22)
            ax.set_ylabel(f"Origin Base ID (N={df_plot.shape[0]})", fontsize=22)

            out_path = os.path.join(stage4_dir, "vis_stage4_logistics_heatmap_base_to_task_full.png")
            plt.savefig(out_path, dpi=300, bbox_inches="tight")
            plt.close(fig)
            print(f"  -> Saved Heatmap: {out_path}")
        except Exception as e:
            print(f"  -> Failed to plot base-to-task heatmap: {e}")
    else:
        print("  -> 'travel_base_to_task.csv' not found. Skipping base-to-task heatmap.")

    task_matrix_file = os.path.join(stage4_dir, "travel_task_to_task.csv")
    if os.path.exists(task_matrix_file):
        try:
            df_travel = pd.read_csv(task_matrix_file, index_col=0)

            # NOTE (logic risk): fillna(0) makes missing/unreachable look like 0 travel time.
            df_plot = df_travel.replace([np.inf, -np.inf], np.nan).fillna(0)

            fig, ax = plt.subplots(figsize=(12, 10))
            sns.heatmap(
                df_plot,
                cmap="YlGnBu",
                cbar_kws={"label": "Travel Time (Hours)"},
                xticklabels=False,
                yticklabels=False,
                ax=ax,
            )
            ax.set_title(
                "Logistics Constraint: Task-to-Task Travel Time (Full Network)",
                fontweight="bold",
                fontsize=24,
                pad=10,
            )
            ax.set_xlabel(f"Destination Substation ID (N={df_plot.shape[1]})", fontsize=22)
            ax.set_ylabel(f"Origin Substation ID (N={df_plot.shape[0]})", fontsize=22)

            out_path = os.path.join(stage4_dir, "vis_stage4_logistics_heatmap_full.png")
            plt.savefig(out_path, dpi=300, bbox_inches="tight")
            plt.close(fig)
            print(f"  -> Saved Heatmap: {out_path}")
        except Exception as e:
            print(f"  -> Failed to plot heatmap: {e}")
    else:
        print("  -> 'travel_task_to_task.csv' not found. Skipping heatmap.")

    # ==================================================================
    # PART 2: Scenario Analysis (Topology Curves + KPI Bars)
    # ==================================================================
    for scen in SCENARIOS:
        print(f"\nProcessing Scenario: {scen}...")

        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        has_data = False
        x_candidates = []

        # --------------------------
        # 2A) Stage 3 Baseline (black dashed)
        # --------------------------
        s3_file = os.path.join(stage3_dir, f"graph_robustness_mean_{scen}.csv")
        if os.path.exists(s3_file):
            try:
                df_s3 = pd.read_csv(s3_file).sort_values("t")

                lcc_series = _safe_lcc_fraction(df_s3)
                if lcc_series is not None:
                    axes[0].plot(
                        df_s3["t"],
                        lcc_series,
                        color="black",
                        ls="--",
                        lw=2,
                        label="Theoretical Limit (Unconstrained)",
                        alpha=0.6,
                        zorder=0,
                    )

                    unfinished = df_s3[lcc_series < 0.999]
                    if not unfinished.empty:
                        x_candidates.append(float(unfinished["t"].max()))

                if "avg_degree" in df_s3.columns:
                    axes[1].plot(
                        df_s3["t"],
                        df_s3["avg_degree"],
                        color="black",
                        ls="--",
                        lw=2,
                        label="Theoretical Limit (Unconstrained)",
                        alpha=0.6,
                        zorder=0,
                    )
            except Exception:
                pass

        # --------------------------
        # 2B) Stage 4 Rules (colored)
        # --------------------------
        for rule in RULES:
            file_path = os.path.join(stage4_dir, f"rule_graphrobustness_{scen}_{rule}.csv")
            if not os.path.exists(file_path):
                continue

            df = pd.read_csv(file_path).sort_values("t")
            st = style_map.get(rule, {"color": "black", "ls": "--", "lw": 1, "label": rule})

            # LCC fraction curve
            if "lcc_fraction" in df.columns:
                axes[0].plot(
                    df["t"],
                    df["lcc_fraction"],
                    color=st["color"],
                    ls=st["ls"],
                    lw=st["lw"],
                    label=st["label"],
                    alpha=0.9,
                )

                unfinished = df[df["lcc_fraction"] < 0.999]
                if not unfinished.empty:
                    x_candidates.append(float(unfinished["t"].max()))

                has_data = True

            # Avg degree curve
            if "avg_degree" in df.columns:
                axes[1].plot(
                    df["t"],
                    df["avg_degree"],
                    color=st["color"],
                    ls=st["ls"],
                    lw=st["lw"],
                    label=st["label"],
                    alpha=0.9,
                )
                has_data = True

        if has_data:
            if x_candidates:
                last_t = max(x_candidates)
                limit_t = min(TIME_LIMIT_HR, last_t * 1.25)
            else:
                limit_t = TIME_LIMIT_HR

            axes[0].set_title(f"Connectivity Recovery (LCC) for {scen}", fontweight="bold", fontsize=18, pad=8)
            axes[0].set_xlabel("Time (Hours)", fontsize=16)
            axes[0].set_ylabel("Giant Component Fraction", fontsize=16)
            axes[0].set_xlim(0, limit_t)
            axes[0].set_ylim(-0.02, 1.05)
            _ordered_legend(axes[0])
            axes[0].grid(True, linestyle="--", alpha=0.5)

            axes[1].set_title(f"Network Density Recovery (Avg Degree) for {scen}", fontweight="bold", fontsize=18, pad=8)
            axes[1].set_xlabel("Time (Hours)", fontsize=16)
            axes[1].set_ylabel("Average Degree (k)", fontsize=16)
            axes[1].set_xlim(0, limit_t)
            _ordered_legend(axes[1])
            axes[1].grid(True, linestyle="--", alpha=0.5)

            fig.set_size_inches(16, 6)
            plt.tight_layout()

            out_path = os.path.join(stage4_dir, f"vis_stage4_topology_dual_{scen}.png")
            plt.savefig(out_path, dpi=300, bbox_inches="tight")
            plt.close(fig)
            print(f"  -> Saved Dual Topology Plot: {out_path}")
        else:
            plt.close(fig)
            print(f"  -> No Stage 4 topology data for scenario {scen}; skipped.")

        # --------------------------
        # 2C) KPI Efficiency Bar Chart
        # --------------------------
        kpi_file = os.path.join(stage4_dir, f"rule_kpis_pop_{scen}.csv")
        if not os.path.exists(kpi_file):
            kpi_file = os.path.join(stage4_dir, f"rule_kpis_system_{scen}.csv")

        if not os.path.exists(kpi_file):
            print(f"  -> KPI file missing for {scen}.")
            continue

        df_kpi = pd.read_csv(kpi_file)

        if "T50" not in df_kpi.columns:
            print(f"  -> KPI file columns missing T50 for {scen}.")
            continue

        rule_col = _detect_rule_col(df_kpi)

        bar_order = [
            "centrality-first",
            "impact-first",
            "betweenness-first",
            "degree-first",
            "closeness-first",
            "hospital-first",
            "random",
        ]

        available_rules = [r for r in bar_order if r in set(df_kpi[rule_col].astype(str))]
        if not available_rules:
            print(f"  -> KPI file has no expected rules for {scen}.")
            continue

        palette = {r: style_map.get(r, {}).get("color", "gray") for r in available_rules}

        fig, ax = plt.subplots(figsize=(7, 5))
        sns.barplot(
            data=df_kpi[df_kpi[rule_col].astype(str).isin(available_rules)],
            x=rule_col,
            y="T50",
            hue=rule_col,
            order=available_rules,
            palette=palette,
            ax=ax,
            edgecolor="black",
            legend=False,
        )

        ax.set_title(f"Recovery Speed ($T_{{50}}$): {scen}", fontweight="bold")
        ax.set_ylabel("Hours to 50% Recovery")
        ax.set_xlabel("")

        # Short display labels (does not affect legend strings elsewhere)
        tick_texts = []
        for t in ax.get_xticklabels():
            s = t.get_text()
            s = s.replace("centrality-first", "Impact (λ2)")
            s = s.replace("impact-first", "Impact")
            s = s.replace("betweenness-first", "Betweenness")
            s = s.replace("degree-first", "Degree")
            s = s.replace("closeness-first", "Closeness")
            s = s.replace("hospital-first", "Hospital")
            s = s.replace("random", "Random")
            tick_texts.append(s)
        ax.set_xticklabels(tick_texts, fontsize=11)

        for c in ax.containers:
            ax.bar_label(c, fmt="%.0f", padding=2)

        plt.tight_layout()
        out_path = os.path.join(stage4_dir, f"vis_stage4_kpi_{scen}.png")
        plt.savefig(out_path, dpi=300, bbox_inches="tight")
        plt.close(fig)
        print("  -> Saved KPI Plot")


# ==============================================================================
# 6️⃣ Stage 6: Equity Analysis (KPI Comparison)
# ==============================================================================

def vis_stage6():
    stage_dir = os.path.join(OUTPUT_ROOT, "Stage 6 Output")
    if not ensure_dir(stage_dir):
        return
    print("--- Visualizing Stage 6 (Equity Gap) ---")

    # Plot style (best-effort)
    try:
        sns.set_style("whitegrid")
        sns.set_context("talk", font_scale=1.1)
    except Exception:
        plt.style.use("ggplot")

    # Fixed input file (Stage 6 summary already known)
    target_file = os.path.join(stage_dir, "recovery_kpis_all_system.csv")
    if not os.path.exists(target_file):
        print(f"  [Skip] Stage 6 KPI file not found: {os.path.basename(target_file)}")
        return

    try:
        df = pd.read_csv(target_file)
    except Exception as e:
        print(f"  [Error] Failed to read Stage 6 CSV: {target_file} ({e})")
        return

    if df.empty:
        print("  [Skip] Stage 6 CSV is empty.")
        return

    # Expect columns produced by the pipeline: scenario, rule, T50
    required = {"scenario", "rule", "T50"}
    if not required.issubset(set(df.columns)):
        print("  [Error] Stage 6 CSV missing required columns.")
        print(f"         Required: {sorted(required)}")
        print(f"         Found:    {list(df.columns)}")
        return

    # Normalize and aggregate (robust to duplicates)
    df["T50"] = pd.to_numeric(df["T50"], errors="coerce")
    df["_scenario_norm"] = df["scenario"].astype(str).str.strip()
    df["_rule_norm"] = (
        df["rule"].astype(str).str.strip().str.lower()
        .str.replace("-", "_", regex=False)
        .str.replace(" ", "_", regex=False)
    )

    agg = (
        df.groupby(["_scenario_norm", "_rule_norm"], as_index=False)["T50"]
          .mean()
          .rename(columns={"T50": "Hours"})
    )

    # Assemble plot data: Pop vs SVI
    scen_list = SCENARIOS if "SCENARIOS" in globals() else sorted(agg["_scenario_norm"].unique())
    pop_key = "stage3_mean"
    svi_key = "stage3_mean_svipop"

    plot_rows = []
    for scen in scen_list:
        scen_str = str(scen)
        sub = agg[agg["_scenario_norm"].str.lower() == scen_str.lower()]
        if sub.empty:
            continue

        pop_val = sub.loc[sub["_rule_norm"] == pop_key, "Hours"]
        svi_val = sub.loc[sub["_rule_norm"] == svi_key, "Hours"]

        if not pop_val.empty and np.isfinite(pop_val.iloc[0]):
            plot_rows.append({
                "Scenario": scen_str,
                "Weighting": "Pop. Weighted (Standard)",
                "Hours": float(pop_val.iloc[0]),
            })

        if not svi_val.empty and np.isfinite(svi_val.iloc[0]):
            plot_rows.append({
                "Scenario": scen_str,
                "Weighting": "SVI Weighted (Vulnerable)",
                "Hours": float(svi_val.iloc[0]),
            })

    if not plot_rows:
        print("  [Warning] No matched rows for Stage3_Mean / Stage3_Mean_SVIpop.")
        print(f"            Available rules: {sorted(agg['_rule_norm'].unique())[:20]} ...")
        return

    plot_df = pd.DataFrame(plot_rows)

    # Scenario ordering (if defined)
    if "SCENARIOS" in globals():
        plot_df["Scenario"] = pd.Categorical(plot_df["Scenario"], SCENARIOS, ordered=True)

    weight_order = ["Pop. Weighted (Standard)", "SVI Weighted (Vulnerable)"]
    palette = {
        "Pop. Weighted (Standard)": "#3498db",
        "SVI Weighted (Vulnerable)": "#e74c3c",
    }

    fig, ax = plt.subplots(figsize=(11, 6))
    sns.barplot(
        x="Scenario",
        y="Hours",
        hue="Weighting",
        data=plot_df,
        hue_order=weight_order,
        palette=palette,
        ax=ax,
        edgecolor="black",
    )

    for container in ax.containers:
        ax.bar_label(container, fmt="%.1f h", padding=3, fontsize=11)

    ax.set_title(
        "Equity Gap: Standard vs. SVI-Weighted System Recovery Time ($T_{50}$)",
        fontsize=18,
        fontweight="bold",
        pad=10,
    )
    ax.set_ylabel("Time to 50% Recovery (Hours)", fontsize=16)
    ax.set_xlabel("Earthquake Scenario", fontsize=16)
    ax.tick_params(axis="x", labelsize=13)
    ax.tick_params(axis="y", labelsize=13)
    ax.grid(axis="y", linestyle="--", alpha=0.4)

    ax.legend(
        title="Metric Type",
        loc="upper left",
        fontsize=13,
        title_fontsize=13,
        framealpha=0.95,
    )

    fig.tight_layout()
    save_plot(fig, stage_dir, "vis_stage6_equity_gap.png")


# ==============================================================================
# 8️⃣ Stage 7: Typology
# ==============================================================================
def vis_stage7(gdf):
    stage_dir = os.path.join(OUTPUT_ROOT, "Stage 7 Output")
    if not ensure_dir(stage_dir):
        return
    print("--- Visualizing Stage 7 ---")

    csv_path = os.path.join(stage_dir, "clusters_labels_final.csv")
    if not os.path.exists(csv_path):
        print("  [Skip] Stage 7 clusters_labels_final.csv not found.")
        return

    df = pd.read_csv(csv_path)
    if df.empty:
        print("  [Skip] Stage 7 CSV is empty.")
        return

    # ------------------------------------------------------------------
    # Helpers
    # ------------------------------------------------------------------
    def _norm_tract_id(s: pd.Series) -> pd.Series:
        """Extract digits and zfill to 11 (FIPS)."""
        out = (
            s.astype(str)
             .str.extract(r"(\d+)")[0]
             .str.zfill(11)
        )
        return out

    def _to_numeric_inplace(frame: pd.DataFrame, cols: list[str]) -> None:
        """Convert selected columns to numeric safely."""
        for c in cols:
            if c in frame.columns:
                frame[c] = pd.to_numeric(frame[c], errors="coerce")

    # Normalize tract_id in df
    if "tract_id" in df.columns:
        df["tract_id"] = _norm_tract_id(df["tract_id"])
    else:
        print("  [Skip] 'tract_id' missing in Stage 7 CSV.")
        return

    # Ensure cluster exists
    if "cluster" not in df.columns:
        print("  [Skip] 'cluster' missing in Stage 7 CSV.")
        return

    # ------------------------------------------------------------------
    # 1) PCA Scatter: PC1 vs PC2 colored by cluster
    # ------------------------------------------------------------------
    if {"PC1", "PC2", "cluster"}.issubset(df.columns):
        fig, ax = plt.subplots(figsize=(10, 8))

        # cluster as category for stable coloring
        df_scatter = df.copy()
        df_scatter["cluster"] = df_scatter["cluster"].astype(str)

        sns.scatterplot(
            data=df_scatter,
            x="PC1",
            y="PC2",
            hue="cluster",
            palette="tab10",
            s=50,
            ax=ax,
            linewidth=0.0,
        )
        ax.set_title("Census Tract Typology Clusters (PC1 vs PC2)", fontsize=16, fontweight="bold")
        ax.set_xlabel("PC1")
        ax.set_ylabel("PC2")
        ax.legend(title="Cluster", loc="best", framealpha=0.95)
        save_plot(fig, stage_dir, "vis_stage7_scatter.png")
    else:
        print("  [Stage 7] PC1/PC2 or cluster missing, skip scatter.")

    # ------------------------------------------------------------------
    # 2) Cluster Map on GIS
    # ------------------------------------------------------------------
    if gdf is None or gdf.empty:
        print("  [Stage 7] gdf is None/empty, skip map.")
    else:
        try:
            g_map = gdf.copy()
            g_map["tract_id"] = _norm_tract_id(g_map["tract_id"])

            df_map = df[["tract_id", "cluster"]].copy()
            df_map["cluster"] = df_map["cluster"].astype(str)

            merged = g_map.merge(df_map, on="tract_id", how="left")

            in_stage = merged["tract_id"].isin(df_map["tract_id"])
            merged = merged.loc[in_stage].copy()
            if merged.empty:
                print("  [Stage 7] No tracts matched between gdf and clusters file; skip map.")
                return

            merged_plot = merged
            try:
                merged_plot = merged.to_crs(epsg=3310)
            except Exception:
                pass

            clusters = sorted(merged_plot["cluster"].dropna().unique().tolist())
            n_clusters = len(clusters)
            if n_clusters == 0:
                print("  [Stage 7] No clusters to map (all NaN).")
                return

            # categorical codes 0..K-1
            merged_plot["_cluster_cat"] = pd.Categorical(
                merged_plot["cluster"], categories=clusters, ordered=True
            )
            merged_plot["_cluster_code"] = merged_plot["_cluster_cat"].cat.codes  # -1 for NaN

            cmap_name = "tab10" if n_clusters <= 10 else "tab20"
            cmap = plt.get_cmap(cmap_name, n_clusters)

            fig, ax = plt.subplots(figsize=(10, 10))

            # plot missing only within in_stage subset (usually tract_id join mismatch)
            missing = merged_plot[merged_plot["_cluster_code"] < 0]
            observed = merged_plot[merged_plot["_cluster_code"] >= 0]

            if not missing.empty:
                missing.plot(
                    ax=ax,
                    color="lightgrey",
                    edgecolor="white",
                    linewidth=0.2,
                    zorder=0,
                )

            observed.plot(
                column="_cluster_code",
                ax=ax,
                cmap=cmap,
                edgecolor="white",
                linewidth=0.2,
                zorder=1,
            )

            bounds_src = observed if not observed.empty else merged_plot
            minx, miny, maxx, maxy = bounds_src.total_bounds
            pad_x = (maxx - minx) * 0.03
            pad_y = (maxy - miny) * 0.03
            ax.set_xlim(minx - pad_x, maxx + pad_x)
            ax.set_ylim(miny - pad_y, maxy + pad_y)
            ax.set_aspect("equal")

            # manual legend (discrete)
            import matplotlib.patches as mpatches
            handles = [
                mpatches.Patch(color=cmap(i), label=str(clusters[i]))
                for i in range(n_clusters)
            ]
            if not missing.empty:
                handles.append(mpatches.Patch(color="lightgrey", label="Missing"))

            ax.legend(
                handles=handles,
                title="Cluster",
                loc="lower left",
                framealpha=0.95,
                fontsize=11,
                title_fontsize=12,
            )

            ax.set_title("Tract Typology Clusters Map", fontsize=16, fontweight="bold", pad=10)
            ax.axis("off")

            save_plot(fig, stage_dir, "vis_stage7_map_clusters.png")

        except Exception as e:
            print(f"  [Stage 7] Failed to draw cluster map: {e}")

    # ------------------------------------------------------------------
    # 3) Heatmap of cluster feature profiles (Z-score across clusters)
    # ------------------------------------------------------------------
    # Convert numeric-like strings to numeric where relevant
    numeric_candidates = [c for c in df.columns if c not in ["tract_id", "cluster"]]
    _to_numeric_inplace(df, numeric_candidates)

    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()

    # ignore
    cols_ignore = {
        "tract_id",
        "cluster",
        "scenario",
        "time_hr",
        "T50",
        "Redundancy_MaxShare",
        "N_subs",
    }
    cols_ignore |= {c for c in numeric_cols if str(c).startswith("PC")}

    # prioritized feature groups
    phys_feats = ["T80", "AUC", "Init_Prob"]
    grid_feats = [
        "Grid_Centrality",
        "Grid_ImpactLambda2",
        "Grid_Betweenness",
        "Redundancy_HHI",
        "Dist_to_CrewYard_km",
    ]
    built_feats = ["Pre_1970_Ratio", "Pop_Density"]

    svi_feats = [c for c in ["SVI_THEME1", "SVI_THEME2", "SVI_THEME3", "SVI_THEME4"] if c in numeric_cols]
    nri_feats = [c for c in ["NRI_RISK_SCORE", "NRI_EAL_SCORE", "NRI_BUILDVALUE"] if c in numeric_cols]

    ordered_feats = []
    for group in [phys_feats, grid_feats, built_feats, svi_feats, nri_feats]:
        for c in group:
            if c in numeric_cols and c not in cols_ignore and c not in ordered_feats:
                ordered_feats.append(c)

    # append remaining numeric cols
    for c in numeric_cols:
        if c not in cols_ignore and c not in ordered_feats:
            ordered_feats.append(c)

    feat_cols = ordered_feats
    if not feat_cols:
        print("  [Stage 7] No valid numeric feature columns for heatmap/KDE.")
        return

    print(f"  Generating Heatmap for features (n={len(feat_cols)}).")

    try:
        grp = df.groupby("cluster")[feat_cols].mean()

        # Z-score across clusters for each feature
        mu = grp.mean(axis=0)
        sd = grp.std(axis=0).replace(0, 1.0)
        grp_z = (grp - mu) / sd

        # readability controls
        n_feat = grp_z.shape[1]
        n_clu = grp_z.shape[0]
        show_annot = (n_feat <= 18 and n_clu <= 12)

        fig_h = max(6, 0.35 * n_feat)
        fig_w = max(10, 0.8 * n_clu + 6)

        fig, ax = plt.subplots(figsize=(fig_w, fig_h))
        sns.heatmap(
            grp_z.T,
            cmap="coolwarm",
            center=0,
            annot=show_annot,
            fmt=".1f",
            ax=ax,
        )
        ax.set_title(
            "Cluster Feature Profiles (Z-score across clusters)",
            fontsize=16,
            fontweight="bold",
            pad=10,
        )
        ax.set_xlabel("Cluster")
        ax.set_ylabel("Feature")
        save_plot(fig, stage_dir, "vis_stage7_heatmap.png")
    except Exception as e:
        print(f"  [Stage 7] Heatmap failed: {e}")

    # ------------------------------------------------------------------
    # 4) KDE Profiles (Facet)
    # ------------------------------------------------------------------
    kde_target_feats = [
        "T80",
        "Init_Prob",
        "Grid_Centrality",
        "Redundancy_HHI",
        "Dist_to_CrewYard_km",
        "Pre_1970_Ratio",
        "Pop_Density",
        "SVI_THEME1",
        "SVI_THEME2",
        "SVI_THEME3",
        "SVI_THEME4",
        "NRI_RISK_SCORE",
        "NRI_EAL_SCORE",
        "NRI_BUILDVALUE",
    ]

    valid_kde_cols = [c for c in kde_target_feats if c in df.columns]
    missing_cols = sorted(set(kde_target_feats) - set(valid_kde_cols))
    if missing_cols:
        print(f"  [Stage 7] Missing KDE columns (skipped): {missing_cols}")

    if not valid_kde_cols:
        print("  [Stage 7] No valid columns found for KDE profiles.")
        return

    df_plot = df.copy()
    df_plot["cluster"] = df_plot["cluster"].astype(str)
    clusters_order = sorted(df_plot["cluster"].dropna().unique())

    n_vars = len(valid_kde_cols)
    n_cols = 3
    n_rows = math.ceil(n_vars / n_cols)

    fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 3.5))
    axes = np.atleast_1d(axes).flatten()

    last_i = -1
    for i, col in enumerate(valid_kde_cols):
        last_i = i
        ax = axes[i]

        sns.kdeplot(
            data=df_plot,
            x=col,
            hue="cluster",
            hue_order=clusters_order,
            common_norm=False,
            fill=False,
            linewidth=2,
            palette="tab10",
            ax=ax,
            warn_singular=False,
        )

        ax.set_title(f"Attribute = {col}", fontweight="bold")
        ax.set_ylabel("Density")
        sns.despine(ax=ax)

        # keep legend only on first subplot
        if i > 0 and ax.get_legend() is not None:
            ax.get_legend().remove()

    # delete unused axes
    for j in range(last_i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout()
    save_plot(fig, stage_dir, "vis_stage7_kde_profiles.png")

# ==============================================================================
# ▶️ Main
# ==============================================================================
if __name__ == "__main__":
    print("=========================================")
    print("   ENHANCED VISUALIZATION GENERATOR      ")
    print("=========================================")
    
    # Load GIS Data
    print("Loading shapefile for mapping...")
    gdf_la = load_shapefile()
    
    vis_stage1(gdf_la)
    vis_stage2()
    vis_stage3(gdf_la)
    vis_stage4()
    vis_stage6()
    vis_stage7(gdf_la)
        
    print("\n✅ All Enhanced Visualizations Completed!")